
2nd exp)
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Lasso, Ridge, SGDRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score
import pandas as pd
diabetes = datasets.load_diabetes()
X = diabetes.data
Y = diabetes.target
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)
models = [
    ("ols", LinearRegression()),
    ("lasso", Lasso(alpha=0.01)),
    ("ridge", Ridge(alpha=1.0)),
    ("gradient", SGDRegressor(max_iter=1000, tol=1e-3)),
    ("polynomial", Pipeline([("poly", PolynomialFeatures(degree=2)), ("linear", LinearRegression())]))
]
result = []
for model_name, model in models:
    model.fit(x_train, y_train)
    y_train_pred = model.predict(x_train)
    y_test_pred = model.predict(x_test)
    mse_train = mean_squared_error(y_train, y_train_pred)
    rmse_train = np.sqrt(mse_train)
    mae_train = mean_absolute_error(y_train, y_train_pred)
    r2_train = r2_score(y_train, y_train_pred)
    explained_var_train = explained_variance_score(y_train, y_train_pred)
    mse_test = mean_squared_error(y_test, y_test_pred)
    rmse_test = np.sqrt(mse_test)
    mae_test = mean_absolute_error(y_test, y_test_pred)
    r2_test = r2_score(y_test, y_test_pred)
    explained_var_test = explained_variance_score(y_test, y_test_pred)
    result.append({
        "model": model_name,
        "MSE_train": mse_train,
        "RMSE_train": rmse_train,
        "MAE_train": mae_train,
        "R2_train": r2_train,
        "explained_variance_train": explained_var_train,
        "MSE_test": mse_test,
        "RMSE_test": rmse_test,
        "MAE_test": mae_test,
        "R2_test": r2_test,
        "explained_variance_test": explained_var_test
    })
result_df = pd.DataFrame(result)
print("Metrics for both train and test data:")
print(result_df)







3rd exp)


import pandas as pd
import math
import numpy as np
data = pd.read_csv("decision.csv")
features = [feat for feat in data]
print(features)
features.remove("answer")
print(features)
class Node:
    def __init__(self):
        self.children = []
        self.value = ""
        self.isLeaf = False
        self.pred = ""
def entropy(examples):
    pos = 0.0
    neg = 0.0
    for _, row in examples.iterrows():
        if row["answer"] == "yes":
            pos += 1
        else:
            neg += 1
    if pos == 0.0 or neg == 0.0:
        return 0.0
    else:
        p = pos / (pos + neg)
        n = neg / (pos + neg)
        return -(p * math.log(p, 2) + n * math.log(n, 2))
def info_gain(examples, attr):
    uniq = np.unique(examples[attr])
    print ("\n",uniq)
    gain = entropy(examples)
    #print ("\n",gain)
    for u in uniq:
        subdata = examples[examples[attr] == u]
        #print ("\n",subdata)
        sub_e = entropy(subdata)
        gain -= (float(len(subdata)) / float(len(examples))) * sub_e
        #print ("\n",gain)
    return gain  
def ID3(examples, attrs):
    root = Node()

    max_gain = 0
    max_feat = ""
    for feature in attrs:
        #print ("\n",examples)
        gain = info_gain(examples, feature)
        if gain > max_gain:
            max_gain = gain
            max_feat = feature
    root.value = max_feat
    #print ("\nMax feature attr",max_feat)
    uniq = np.unique(examples[max_feat])
    #print ("\n",uniq)
    for u in uniq:
        #print ("\n",u)
        subdata = examples[examples[max_feat] == u]
        #print ("\n",subdata)
        if entropy(subdata) == 0.0:
            newNode = Node()
            newNode.isLeaf = True
            newNode.value = u
            newNode.pred = np.unique(subdata["answer"])
            root.children.append(newNode)
        else:
            dummyNode = Node()
            dummyNode.value = u
            new_attrs = attrs.copy()
            new_attrs.remove(max_feat)
            child = ID3(subdata, new_attrs)
            dummyNode.children.append(child)
            root.children.append(dummyNode)

    return root
def printTree(root: Node, depth=0):
    for i in range(depth):
        print("\t", end="")
    print(root.value, end="")
    if root.isLeaf:
        print(" -> ", root.pred)
    print()
    for child in root.children:
        printTree(child, depth + 1)
def classify(root: Node, new):
    for child in root.children:
        if child.value == new[root.value]:
            if child.isLeaf:
                print ("Predicted Label for new example", new," is:", child.pred)
                exit
            else:
                classify (child.children[0], new)
root = ID3(data, features)
print("Decision Tree is:")
printTree(root)
print ("------------------")
new = {"outlook":"sunny", "temperature":"hot", "humidity":"normal", "wind":"strong"}
classify (root, new)





4th exp)

import pandas as pd
import numpy as np
data=pd.read_csv("data.csv")
print(data.shape)
print(data.head(5))
X = data.iloc[:, :-1].values
Y = data.iloc[:, -1].values
print(X[5])
print(Y[0])
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(random_state = 0)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix: ")
print(cm)
print("Accuracy of the Model: {0}%".format(accuracy_score(y_test, y_pred)*100))
age = int(input("Enter New Customer Age: "))
sal = int(input("Enter New Customer Salary: "))
newCust = [[age,sal]]
result = model.predict(sc.transform(newCust))
print(result)
if result == 1:
  print("Customer will Buy")
else:
  print("Customer won't Buy")









5th exp)
# NAIVE BAYES ----------------------------------------------------------------------------------------------------------------------
# importing the required modules

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.preprocessing import LabelEncoder

df=pd.read_csv("/content/drug200.csv")
print(df.head(3))
print(df.columns)

# converting catergorical data to numerical

label_encoder = LabelEncoder()

for col in df.columns:
  df[col]=label_encoder.fit_transform(df[col])

print(df.head(3))

# splitting target and data

X_data = df.iloc[:,:-1].values
Y_data = df.iloc[:,-1].values

print(X_data)
print(Y_data)

# splitting to train and test

x_train,x_test,y_train,y_test = train_test_split(X_data,Y_data,test_size=0.3,random_state=42)

# fitting into the model

gauss_nb_model = GaussianNB()
gauss_nb_model.fit(x_train,y_train)


# predictions
y_predictions = gauss_nb_model.predict(x_test)

for i in range(y_predictions.size):
  print(y_predictions[i],y_test[i])

# accuracy score
accr = accuracy_score(y_test,y_predictions)
print(f"ACCURACY : {accr*100}%")

# confusion matrix
cm=confusion_matrix(y_test,y_predictions)
print("CONFUSION MATRIX :\n",cm)

tp=cm[0][0]
fp=cm[0][1]
fn=cm[1][0]

precision = tp/(tp+fp)
recall = tp/(tp+fn)
print(f"PRECISION : {precision*100}%")
print(f"RECALL : {recall*100}%")

6th exp)
# ENSEMBLING METHODS-----------------------------------------------------------------------------------------------------------------
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.metrics import accuracy_score

df=pd.read_csv("/content/drug200.csv")

encoded_val=[]
cols=['Sex','BP','Cholesterol','Drug']
le = LabelEncoder()
for col in cols:
  df[col]=le.fit_transform(df[col])


X=df.iloc[:,:-1].values
y=df.iloc[:,-1].values


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


models = [
    ["Decision Tree" , DecisionTreeClassifier(random_state=42)],
    ["Random Forest" , RandomForestClassifier(random_state=42)],
    ["Bagging" , BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=10, random_state=42)],
    ["Boosting (AdaBoost)" ,AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=50, random_state=42)],
    ["Voting" , VotingClassifier(estimators=[
    ('dt', DecisionTreeClassifier(random_state=42)),
    ('rf', RandomForestClassifier(random_state=42)),
    ('bag', BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=10, random_state=42)),
    ('boost', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=50, random_state=42))
    ], voting='hard')]
]


answer = []
for model_name,model in models:
  model.fit(X_train,y_train)
  y_predictions = model.predict(X_test)
  acc = accuracy_score(y_test,y_predictions)

  answer.append([model_name,acc*100])


print(answer)
d = pd.DataFrame(answer,columns=["Model","Accuracy"])
print(d)


7th exp)
# SVm-----------------------------------------------------------------------------------------------------------------------------
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.svm import SVC

data = load_digits()

X= data.data
y= data.target

x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.25 ,random_state=0)


model = SVC(gamma=0.001,C=100)
model.fit(x_train,y_train)
y_pred = model.predict(x_test)

#printing the predicted and the actual values
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))


# printing the accuracy, precision and recall
accr = accuracy_score(y_test,y_pred)
cm = confusion_matrix(y_test,y_pred)

tp,fp,fn = cm[0][0],cm[0][1],cm[1][0]

prec = tp/(tp+fp)
recall = tp/(tp+fn)

print(f"ACCURACY : {accr*100}%")
print(f"PRECISION : {prec*100}%")
print(f"RECALL : {recall*100}%")



fig1, axes1 = plt.subplots(8, 8, figsize=(12,12))
for axis, images, prediction in zip(axes1.flat, x_test, y_pred):
  images = images.reshape(8, 8)  # Reshape back to 8x8 for display
  axis.imshow(images,cmap='binary')
  axis.set(xticks=[],yticks=[])
  axis.set_title(f'Prediction: {prediction}')



8th exp)
# Gradient Descent------------------------------------------------------------------------------------------------------------------
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Generate some random data for demonstration
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.rand(100, 1)

# Add a bias term to the input features
X_b = np.c_[np.ones((100, 1)), X]

# Convert NumPy arrays to TensorFlow tensors
X_tensor = tf.constant(X_b, dtype=tf.float32)
y_tensor = tf.constant(y, dtype=tf.float32)

# Set random seed for reproducibility
tf.random.set_seed(42)

# Initialize random values for the model parameters
theta = tf.Variable(tf.random.normal(shape=(2, 1)))

# Define the linear regression model
def linear_regression(X, theta):
    return tf.matmul(X, theta)

# Define the mean squared error (MSE) loss function
def mean_squared_error(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

# Define the learning rate and the number of iterations
learning_rate = 0.01
n_iterations = 1000

# Implement the gradient descent algorithm
for iteration in range(n_iterations):
    with tf.GradientTape() as tape:
        y_pred = linear_regression(X_tensor, theta)
        loss = mean_squared_error(y_tensor, y_pred)

    gradients = tape.gradient(loss, [theta])[0]
    theta.assign(theta - learning_rate * gradients)

# Print the optimized parameters
print("Optimized theta:")
print(theta.numpy())

# Plot the original data and the linear regression line
plt.scatter(X, y, label="Original data")
plt.plot(X, theta[0] + theta[1] * X, color='red', label="Linear regression")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.show()



9th exp)
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from scipy import stats
import pylab as pl

# Display HTML
from IPython.display import display, HTML
data =  pd.read_csv('iris.csv')
data.head()
print(data.shape)
data.info()
X = data.iloc[:, :-1].values    #   X -> Feature Variables
y = data.iloc[:, -1].values #   y ->  Target
# Splitting the data into Train and Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)
XL = data.iloc[:, :-1].values    #   X -> Feature Variables
yL = data.iloc[:, -1].values #   y ->  Target

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
Y_train= le.fit_transform(yL)

print(Y_train)
# This is only for Linear Regretion
X_trainL, X_testL, y_trainL, y_testL = train_test_split(XL, Y_train, test_size = 0.3, random_state = 0)
from sklearn.linear_model import LinearRegression

modelLR = LinearRegression()
modelLR.fit(X_trainL, y_trainL)

Y_pred = modelLR.predict(X_testL)
from sklearn import metrics
#calculating the residuals
print('y-intercept             :' , modelLR.intercept_)
print('beta coefficients       :' , modelLR.coef_)
print('Mean Abs Error MAE      :' ,metrics.mean_absolute_error(y_testL,Y_pred))
print('Mean Sqrt Error MSE     :' ,metrics.mean_squared_error(y_testL,Y_pred))
print('Root Mean Sqrt Error RMSE:' ,np.sqrt(metrics.mean_squared_error(y_testL,Y_pred)))
print('r2 value                :' ,metrics.r2_score(y_testL,Y_pred))
# Decision Tree's
print("Decision Tree\n==============================")
from sklearn.tree import DecisionTreeClassifier
Model = DecisionTreeClassifier()
Model.fit(X_train, y_train)
y_pred = Model.predict(X_test)
# Summary of the predictions made by the classifier
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
# Accuracy score
print('accuracy is',accuracy_score(y_pred,y_test))
# RandomForestClassifier
print("Random Forest Classifier\n===================================")
from sklearn.ensemble import RandomForestClassifier
Model=RandomForestClassifier(max_depth=2)
Model.fit(X_train,y_train)
y_pred=Model.predict(X_test)

# Summary of the predictions made by the classifier
print(classification_report(y_test,y_pred))
print(confusion_matrix(y_pred,y_test))
#Accuracy Score
print('accuracy is ',accuracy_score(y_pred,y_test))
# LogisticRegression
print("LogisticRegression\n===================================")
from sklearn.linear_model import LogisticRegression
Model = LogisticRegression()
Model.fit(X_train, y_train)

y_pred = Model.predict(X_test)

# Summary of the predictions made by the classifier
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
# Accuracy score
print('accuracy is',accuracy_score(y_pred,y_test))
# K-Nearest Neighbours
print("K-Nearest Neighbours\n===================================")
from sklearn.neighbors import KNeighborsClassifier

Model = KNeighborsClassifier(n_neighbors=8)
Model.fit(X_train, y_train)

y_pred = Model.predict(X_test)

# Summary of the predictions made by the classifier
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
# Accuracy score

print('accuracy is',accuracy_score(y_pred,y_test))
# Naive Bayes
print("Naive Bayes\n===================================")
from sklearn.naive_bayes import GaussianNB
Model = GaussianNB()
Model.fit(X_train, y_train)

y_pred = Model.predict(X_test)

# Summary of the predictions made by the classifier
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
# Accuracy score
print('accuracy is',accuracy_score(y_pred,y_test))
# Support Vector Machine
print("Support Vector Machine\n===================================")
from sklearn.svm import SVC

Model = SVC()
Model.fit(X_train, y_train)

y_pred = Model.predict(X_test)

# Summary of the predictions made by the classifier
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
# Accuracy score

print('accuracy is',accuracy_score(y_pred,y_test))



10th exp)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import mglearn
df = pd.read_csv("Mall_Customers.csv")
df.head()
df.columns = ['customer_ID','gender','age','annual_income','spending_score']
df.head()
df.shape
df.duplicated().any()
df.isnull().any()
df = df.set_index(['customer_ID'])
df.head()
X = df.drop(['gender'], axis=1)
X.head()
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
clusters = []
ss = []
#Calculate all the sum of within-cluster variance for n_clusters from 2 to 14
for i in range(2,15):
    km = KMeans(n_clusters = i)
    km.fit(X)
    clusters.append(km.inertia_)
    ss.append(silhouette_score(X, km.labels_, metric='euclidean'))
fig, ax = plt.subplots(figsize=(12, 8))
sns.lineplot(x=list(range(2, 15)), y=clusters, ax=ax)
ax.set_title('Searching for Elbow')
ax.set_xlabel('Clusters')
ax.set_ylabel('Inertia')

# Annotate arrow
ax.annotate('Possible Elbow Point', xy=(5, 80000), xytext=(5, 150000), xycoords='data',
             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))

plt.show()
km5 = KMeans(n_clusters=5).fit(X)
X['Labels'] = km5.labels_
fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(121)
sns.scatterplot(x=X['annual_income'], y=X['spending_score'], hue=X['Labels'])
#,palette=sns.color_palette('hls', 5))
ax.set_title('KMeans with 5 Clusters')
ax.legend(loc='center right')

from sklearn.cluster import AgglomerativeClustering

aggloclus = AgglomerativeClustering(n_clusters=5, linkage='complete').fit(X)

labels = aggloclus.labels_
fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(121)
sns.scatterplot(x=X['annual_income'],y=X['spending_score'], hue=X['Labels'])
ax.set_title('Agglomerative 5 clusters with complete linkage')
ax.legend(loc='center right')
plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
dbscan = DBSCAN(eps = 0.7)
clusters = dbscan.fit_predict(X_scaled)
length = len(np.unique(clusters))
fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(121)
sns.scatterplot(x=X['annual_income'], y=X['spending_score'], hue=X['Labels'])
ax.set_title('DBSCAN with 5 Clusters')
ax.legend(loc='center right')
plt.show()
